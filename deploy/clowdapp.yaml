apiVersion: v1
kind: Template
metadata:
  name: trino-template
  annotations:
    openshift.io/display-name: "Trino"
    openshift.io/long-description: "This template defines resources needed to deploy and run the Trino."
    openshift.io/provider-display-name: "Red Hat, Inc."
labels:
  app: trino
  template: trino
objects:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: trino-config
    labels:
      app: trino
  data:
    initialize_trino.sh: |
      #!/bin/bash
      set -e
      if [[ ! -z "${ACG_CONFIG}" ]]; then
        export DATABASE_HOST=$(jq -r '.database.hostname' ${ACG_CONFIG})
        export DATABASE_PORT=$(jq -r '.database.port' ${ACG_CONFIG})
        export DATABASE_USER=$(jq -r '.database.username' ${ACG_CONFIG})
        export DATABASE_PASSWORD=$(jq -r '.database.password' ${ACG_CONFIG})
        export DATABASE_NAME=$(jq -r '.database.name' ${ACG_CONFIG})
        export DATABASE_SSLMODE=$(jq -r '.database.sslMode' ${ACG_CONFIG})
        if [[ $DATABASE_SSLMODE = "null" ]]; then
          unset DATABASE_SSLMODE
        fi
        certString=$(jq -r '.database.rdsCa' ${ACG_CONFIG})
        if [[ $certString != "null" ]]; then
          temp_file=$(mktemp)
          echo "RDS Cert Path: $temp_file"
          echo "$certString" > $temp_file
          export PGSSLROOTCERT=$temp_file
        fi
      fi
      echo "Copy config files to ${TRINO_HOME}/"
      cp -v -L -r -f /trino-etc/* ${TRINO_HOME}/
      if [ ! -f ${TRINO_HOME}/catalog/postgres.properties ]; then
        echo "Creating trino connector configuration..."
        echo "connector.name=postgresql" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "connection-url=jdbc:postgresql://${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_NAME}" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "connection-user=${DATABASE_USER}" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "connection-password=${DATABASE_PASSWORD}" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "postgresql.array-mapping=AS_ARRAY" >> ${TRINO_HOME}/catalog/postgres.properties
      fi
    entrypoint.sh: |
      #!/bin/bash
      function importCert() {
        PEM_FILE=$1
        PASSWORD=$2
        KEYSTORE=$3
        # number of certs in the PEM file
        CERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)
        # For every cert in the PEM file, extract it and import into the JKS keystore
        # awk command: step 1, if line is in the desired cert, print the line
        #              step 2, increment counter when last line of cert is found
        for N in $(seq 0 $(($CERTS - 1))); do
          ALIAS="${PEM_FILE%.*}-$N"
          cat $PEM_FILE |
            awk "n==$N { print }; /END CERTIFICATE/ { n++ }" |
            keytool -noprompt -import -trustcacerts \
                    -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD
        done
      }
      set -e

      if [[ ! -z "${ACG_CONFIG}" ]]; then
        export AWS_ACCESS_KEY_ID=$(jq -r '.objectStore.buckets[0].accessKey' ${ACG_CONFIG})
        export AWS_SECRET_ACCESS_KEY=$(jq -r '.objectStore.buckets[0].secretKey' ${ACG_CONFIG})
        export S3_BUCKET_NAME=$(jq -r '.objectStore.buckets[0].requestedName' ${ACG_CONFIG})

        OBJECTSTORE_HOST=$(jq -r '.objectStore.hostname' ${ACG_CONFIG})
        OBJECTSTORE_PORT=$(jq -r '.objectStore.port' ${ACG_CONFIG})
        OBJECTSTORE_TLS=$(jq -r '.objectStore.tls' ${ACG_CONFIG})

        export URI_PREFIX=https
        if [[ $OBJECTSTORE_TLS == *"false"* ]]; then
          export URI_PREFIX=http
        fi

        S3_ENDPOINT="${URI_PREFIX}://${OBJECTSTORE_HOST}"
        if [[ -n "${OBJECTSTORE_PORT}" ]] && [[ "${OBJECTSTORE_PORT}" != "null" ]]; then
          S3_ENDPOINT="${S3_ENDPOINT}:${OBJECTSTORE_PORT}"
        fi
        export S3_ENDPOINT
      fi
      # always add the openshift service-ca.crt if it exists
      if [ -a /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt ]; then
        echo "Adding /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt to $JAVA_HOME/lib/security/cacerts"
        importCert /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt changeit $JAVA_HOME/lib/security/cacerts
      fi
      # add node id to node config
      NODE_CONFIG="${TRINO_HOME}/node.properties"
      # ensure there's a newline between the last item in the config and what we add
      echo "" >> $NODE_CONFIG
      if ! grep -q -F 'node.id' "$NODE_CONFIG"; then
        NODE_ID="node.id=$MY_NODE_ID"
        echo "Adding $NODE_ID to $NODE_CONFIG"
        echo "$NODE_ID" >> "$NODE_CONFIG"
      fi
      CORE_SITE=/hadoop-config/core-site.xml
      mkdir -p ${TRINO_HOME}/hadoop-config
      if [[ -f ${CORE_SITE} ]]; then
        echo "Configuring core-site.xml"
        cat ${CORE_SITE} | sed "s#XXX_S3ENDPOINT_XXX#${S3_BUCKET_NAME}/${S3_DATA_DIR}#" > ${TRINO_HOME}/hadoop-config/core-site.xml
      fi
      # add AWS creds to hive catalog properties
      HIVE_CATALOG_CONFIG="${TRINO_HOME}/catalog/hive.properties"
      # ensure there's a newline between the last item in the config and what we add
      echo "" >> $HIVE_CATALOG_CONFIG
      if ! grep -q -F 'hive.s3.aws-access-key' "$HIVE_CATALOG_CONFIG"; then
        echo "Adding hive.s3.aws-access-key and hive.s3.aws-secret-key to $HIVE_CATALOG_CONFIG"
        echo "hive.s3.aws-access-key=$AWS_ACCESS_KEY_ID" >> "$HIVE_CATALOG_CONFIG"
        echo "hive.s3.aws-secret-key=$AWS_SECRET_ACCESS_KEY" >> "$HIVE_CATALOG_CONFIG"
        echo "hive.s3.endpoint=$S3_ENDPOINT" >> "$HIVE_CATALOG_CONFIG"
        echo "hive.s3.ssl.enabled=$OBJECTSTORE_TLS" >> "$HIVE_CATALOG_CONFIG"
      fi

      # add UID to /etc/passwd if missing
      if ! whoami &> /dev/null; then
          if test -w /etc/passwd || stat -c "%a" /etc/passwd | grep -qE '.[267].'; then
              echo "Adding user ${USER_NAME:-trino} with current UID $(id -u) to /etc/passwd"
              # Remove existing entry with user first.
              # cannot use sed -i because we do not have permission to write new
              # files into /etc
              sed  "/${USER_NAME:-trino}:x/d" /etc/passwd > /tmp/passwd
              # add our user with our current user ID into passwd
              echo "${USER_NAME:-trino}:x:$(id -u):0:${USER_NAME:-trino} user:${HOME}:/sbin/nologin" >> /tmp/passwd
              # overwrite existing contents with new contents (cannot replace the
              # file due to permissions)
              cat /tmp/passwd > /etc/passwd
              rm /tmp/passwd
          fi
      fi
      exec "$@"
    jvm.config: |-
      -server
      -Xmx16G
      -XX:+UseContainerSupport
      -XX:-UseBiasedLocking
      -XX:+UseG1GC
      -XX:G1HeapRegionSize=32M
      -XX:+ExplicitGCInvokesConcurrent
      -XX:+ExitOnOutOfMemoryError
      -XX:ErrorFile=/data/trino/logs/java_error%p.log
      -XX:+HeapDumpOnOutOfMemoryError
      -XX:HeapDumpPath=/data/trino/logs/heap_dump.bin
      -XX:-OmitStackTraceInFastThrow
      -XX:ReservedCodeCacheSize=512M
      -XX:PerMethodRecompilationCutoff=10000
      -XX:PerBytecodeRecompilationCutoff=10000
      -Djdk.attach.allowAttachSelf=true
      -Djdk.nio.maxCachedBufferSize=2000000
      -verbose:gc
      -Xlog:gc*:/data/trino/logs/gc.log
      -javaagent:/usr/lib/trino/jmx_exporter.jar=9000:/etc/trino/catalog/config.yaml
      -Dcom.sun.management.jmxremote
      -Dcom.sun.management.jmxremote.local.only=false
      -Dcom.sun.management.jmxremote.ssl=false
      -Dcom.sun.management.jmxremote.authenticate=false
      -Dcom.sun.management.jmxremote.port=10000
      -Dcom.sun.management.jmxremote.rmi.port=10000
      -Djava.rmi.server.hostname=127.0.0.1
    config.properties.coordinator: |-
      coordinator=true
      node-scheduler.include-coordinator=true
      http-server.http.port=8000
      discovery-server.enabled=true
      discovery.uri=http://trino-coordinator:8000
      query.max-length=10000000
      query.max-memory=${QUERY_MAX_MEMORY}
      query.max-memory-per-node=${QUERY_MAX_MEMORY_PER_NODE}
      query.max-total-memory-per-node=${QUERY_MAX_TOTAL_MEMORY_PER_NODE}
      web-ui.authentication.type=fixed
      web-ui.user=trino
    config.properties.worker: |-
      coordinator=false
      node-scheduler.include-coordinator=true
      http-server.http.port=8000
      discovery.uri=http://trino-coordinator:8000
      query.max-length=10000000
      query.max-memory=${QUERY_MAX_MEMORY}
      query.max-memory-per-node=${QUERY_MAX_MEMORY_PER_NODE}
      query.max-total-memory-per-node=${QUERY_MAX_TOTAL_MEMORY_PER_NODE}
    # log.properties: |-
    #   com.facebook.trino=INFO
    node.properties: |-
      node.data-dir=/data/trino
      node.environment=${ENV_NAME}
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: trino-config-catalog
    labels:
      app: trino
    config.yaml: |-
      ---
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true
      attrNameSnakeCase: false
      rules:
        # capture percentile and set quantile label
        - pattern: 'trino.plugin.hive<type=(.+), name=hive><>(.+AllTime).P(\d+): (.*)'
          name: 'trino_hive_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        # match non-percentiles
        - pattern: 'trino.plugin.hive<type=(.+), name=hive><>(.+AllTime.+): (.*)'
          name: 'trino_hive_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          # counts
        - pattern: 'trino.plugin.hive<type=(.+), name=hive><>(.+TotalCount.*): (.*)'
          name: 'trino_hive_$1_$2_total'
          type: COUNTER
        # capture percentile and set quantile label
        - pattern: 'trino.plugin.hive.s3<type=(.+), name=hive><>(.+AllTime).P(\d+): (.*)'
          name: 'trino_hive_s3_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        # match non-percentiles
        - pattern: 'trino.plugin.hive.s3<type=(.+), name=hive><>(.+AllTime.+): (.*)'
          name: 'trino_hive_s3_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          # counts
        - pattern: 'trino.plugin.hive.s3<type=(.+), name=hive><>(.+TotalCount.*): (.*)'
          name: 'trino_hive_s3_$1_$2_total'
          type: COUNTER
        # capture percentile and set quantile label
        - pattern: 'trino.plugin.hive.metastore.thrift<type=(.+), name=hive><>(.+AllTime).P(\d+): (.*)'
          name: 'trino_hive_metastore_thrift_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        # match non-percentiles
        - pattern: 'trino.plugin.hive.metastore.thrift<type=(.+), name=hive><>(.+AllTime.+): (.*)'
          name: 'trino_hive_metastore_thrift_$1_$2_count_seconds'
          type: GAUGE
          valueFactor: 0.001
        # counts
        - pattern: 'trino.plugin.hive.metastore.thrift<type=(.+), name=hive><>(.+TotalCount.*): (.*)'
          name: 'trino_hive_metastore_thrift_$1_$2_total'
          type: COUNTER
        - pattern: 'trino.execution<name=(.+)><>(.+AllTime).P(\d+): (.*)'
          name: 'trino_execution_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        - pattern: 'trino.execution<name=(.+)><>(.+AllTime.+): (.*)'
          name: 'trino_execution_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
        # counts
        - pattern: 'trino.execution<name=(.+)><>(.+TotalCount.*): (.*)'
          name: 'trino_execution_$1_$2_total'
          type: COUNTER
        - pattern: 'trino.memory<type=(.*), name=(.*)><>(.+): (.*)'
          name: 'trino_memory_$1_$2_$3'
          type: GAUGE
        - pattern: 'trino.failuredetector<name=HeartbeatFailureDetector><>ActiveCount: (.*)'
          name: 'trino_heartbeatdetector_activecount'
          type: GAUGE
    hive.properties: |-
      connector.name=hive-hadoop2
      hive.allow-drop-table=true
      hive.allow-rename-table=true
      hive.collect-column-statistics-on-write=true
      hive.compression-codec=SNAPPY
      hive.config.resources=/etc/trino/hadoop-config/core-site.xml
      hive.hdfs.authentication.type=NONE
      hive.metastore.authentication.type=NONE
      hive.metastore.uri=thrift://hive-metastore:8000
      hive.parquet.use-column-names=true
      hive.s3.path-style-access=true
      hive.s3.sse.enabled=true
      hive.storage-format=Parquet
    blackhole.propeties: |-
      connector.name=blackhole
    jmx.properties: |-
      connector.name=jmx
    memory.properties: |-
      connector.name=memory
    tpcds.properties: |-
      connector.name=tpcds
    tpch.properties: |-
      connector.name=tpch

- apiVersion: cloud.redhat.com/v1alpha1
  kind: ClowdApp
  metadata:
    name: trino
  spec:
    envName: ${ENV_NAME}
    deployments:
    - name: coordinator
      minReplicas: ${{COORDINATOR_REPLICAS}}
      webServices:
        public:
          enabled: true
        metrics:
          enabled: true
      podSpec:
        image: ${IMAGE}:${IMAGE_TAG}
        initContainers:
          - command:
              - /trino-common/initialize_trino.sh
            inheritEnv: true
        command:
          - /trino-common/entrypoint.sh
        args:
          - /opt/trino/trino-server/bin/launcher
          - run
        env:
          - name: MY_NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: S3_DATA_DIR
            value: 'data'
          - name: QUERY_MAX_MEMORY
            value: ${QUERY_MAX_MEMORY}
          - name: QUERY_MAX_MEMORY_PER_NODE
            value: ${QUERY_MAX_MEMORY_PER_NODE}
          - name: QUERY_MAX_TOTAL_MEMORY_PER_NODE
            value: ${QUERY_MAX_TOTAL_MEMORY_PER_NODE}
        resources:
          limits:
            cpu: ${COORDINATOR_CPU_LIMIT}
            memory: ${COORDINATOR_MEMORY_LIMIT}
          requests:
            cpu: ${COORDINATOR_CPU_REQUEST}
            memory: ${COORDINATOR_MEMORY_REQUEST}
        livenessProbe:
          failureThreshold: 3
          tcpSocket:
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /ui/
            port: 8000
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
          timeoutSeconds: 5
        volumes:
          - name: trino-config
            configMap:
              name: trino-config
          - name: trino-config-catalog
            configMap:
              name: trino-config-catalog
          - name: trino-etc
            emptyDir: {}
          - name: trino-data
            emptyDir: {}
          - name: trino-logs
            emptyDir: {}
          - name: hadoop-config
            configMap:
              name: hadoop-clowder-config
              defaultMode: 420
        volumeMounts:
          - name: trino-config
            mountPath: /etc/trino
          - name: trino-config-catalog
            mountPath: /etc/trino/catalog
          - name: trino-data
            mountPath: /data/trino
          - name: trino-logs
            mountPath: /data/trino/logs
          - name: hadoop-config
            mountPath: /etc/trino/hadoop-config
    - name: worker
      minReplicas: ${{WORKER_REPLICAS}}
      webServices:
        public:
          enabled: true
        metrics:
          enabled: true
      podSpec:
        image: ${IMAGE}:${IMAGE_TAG}
        initContainers:
          - command:
              - /trino-common/initialize_trino.sh
            inheritEnv: true
        command:
          - /trino-common/entrypoint.sh
        args:
          - /opt/trino/trino-server/bin/launcher
          - run
        env:
          - name: MY_NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: S3_DATA_DIR
            value: 'data'
          - name: QUERY_MAX_MEMORY
            value: ${QUERY_MAX_MEMORY}
          - name: QUERY_MAX_MEMORY_PER_NODE
            value: ${QUERY_MAX_MEMORY_PER_NODE}
          - name: QUERY_MAX_TOTAL_MEMORY_PER_NODE
            value: ${QUERY_MAX_TOTAL_MEMORY_PER_NODE}
        resources:
          limits:
            cpu: ${WORKER_CPU_LIMIT}
            memory: ${WORKER_MEMORY_LIMIT}
          requests:
            cpu: ${WORKER_CPU_REQUEST}
            memory: ${WORKER_MEMORY_REQUEST}
        livenessProbe:
          failureThreshold: 3
          tcpSocket:
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 3
        readinessProbe:
          httpGet:
            path: /
            port: 9000
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
          timeoutSeconds: 3
        volumes:
          - name: trino-config
            configMap:
              name: trino-config
          - name: trino-config-catalog
            configMap:
              name: trino-config-catalog
          - name: trino-etc
            emptyDir: {}
          - name: trino-data
            emptyDir: {}
          - name: trino-logs
            emptyDir: {}
          - name: hadoop-config
            configMap:
              name: hadoop-clowder-config
              defaultMode: 420
        volumeMounts:
          - name: trino-config
            mountPath: /etc/trino
          - name: trino-config-catalog
            mountPath: /etc/trino/catalog
          - name: trino-data
            mountPath: /data/trino
          - name: trino-logs
            mountPath: /data/trino/logs
          - name: hadoop-config
            mountPath: /etc/trino/hadoop-config
    objectStore:
    - ${S3_BUCKET_NAME}
    database:
      sharedDbAppName: koku
    dependencies:
      - koku


parameters:
- description: Initial amount of memory the Django container will request.
  displayName: Memory Request
  name: COORDINATOR_MEMORY_REQUEST
  required: true
  value: 2Gi
- description: Maximum amount of memory the Django container can use.
  displayName: Memory Limit
  name: COORDINATOR_MEMORY_LIMIT
  required: true
  value: 4Gi
- description: Initial amount of cpu the Django container will request.
  displayName: CPU Request
  name: COORDINATOR_CPU_REQUEST
  required: true
  value: 200m
- description: Maximum amount of cpu the Django container can use.
  displayName: CPU Limit
  name: COORDINATOR_CPU_LIMIT
  required: true
  value: 500m
- description: Initial amount of memory the Django container will request.
  displayName: Memory Request
  name: WORKER_MEMORY_REQUEST
  required: true
  value: 2Gi
- description: Maximum amount of memory the Django container can use.
  displayName: Memory Limit
  name: WORKER_MEMORY_LIMIT
  required: true
  value: 4Gi
- description: Initial amount of cpu the Django container will request.
  displayName: CPU Request
  name: WORKER_CPU_REQUEST
  required: true
  value: 500m
- description: Maximum amount of cpu the Django container can use.
  displayName: CPU Limit
  name: WORKER_CPU_LIMIT
  required: true
  value: '1'
- description: Number of replicas for the coordinator
  displayName: Coordinator replica count
  name: COORDINATOR_REPLICAS
  required: true
  value: '1'
- description: Number of replicas for the worker
  displayName: Worker replica count
  name: WORKER_REPLICAS
  required: true
  value: '3'
- description: Image name
  name: IMAGE
  value: quay.io/cloudservices/ubi-trino
  required: true
- description: Image tag
  displayName: Image tag
  name: IMAGE_TAG
  value: '348-002'
  required: true
- description: Max Memory for a query
  displayName: query.max-memory
  name: QUERY_MAX_MEMORY
  value: '8GB'
  required: true
- description: Max Memory per node for a query
  displayName: query.max-memory-per-node
  name: QUERY_MAX_MEMORY_PER_NODE
  value: '1GB'
  required: true
- description: Max Total Memory for a query
  displayName: query.max-total-memory-per-node
  name: QUERY_MAX_TOTAL_MEMORY_PER_NODE
  value: '2GB'
  required: true
- name: ENV_NAME
  required: false
- name: S3_BUCKET_NAME
  value: 'hccm-s3'
